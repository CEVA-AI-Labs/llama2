QAT:
    fully_quantized: False
    device: 'cuda'
    data_quantization:
      status: Off
      bits: 8
      custom_bits: {'lm_head': 16}
      symmetric: On
      per_channel: True
      quantization_mode: static
      observer: 'MovingAverage'
    weights_quantization:
      status: Off
      bits: 4
      custom_bits: {'lm_head': 16}
      symmetric: On
      per_channel: True
      quant_and_freeze: False



    custom_quantization:

      - source_module: torch.nn.Linear
        destination_module: LinearTrueQuant
        params:
          weights_quantizer:
            weight_per_channel: True
            offline_quant_weights: True
          act_quantizer:
            act_dynamic: True
            observer: MinMax

      - source_module: torch.nn.SiLU
        destination_module: compFlowSilU

      - source_module: LlamaRMSNorm
        destination_module: LlamaTrueRMSNorm

      - source_module: LiteMLMatmul
        source_name: matmul_qkt
        destination_module: compFlowMatMulqkt

      - source_module: LiteMLMatmul
        source_name: matmul_pv
        destination_module: compFlowMatMulPV

      - source_module: torch.nn.Softmax
        destination_module: compFlowSoftmax



