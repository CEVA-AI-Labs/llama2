QAT:
    fully_quantized: False
    device: 'cuda'
    data_quantization:
      status: On
      bits: 8
      custom_bits: {}
      symmetric: On
      per_channel: True
      quantization_mode: 'static'
      observer: 'MovingAverage'
    weights_quantization:
      status: On
      bits: 8
      custom_bits: {}
      symmetric: On
      per_channel: False
      quant_and_freeze: True

    custom_quantization:
      - source_module: torch.nn.Linear
        destination_module: LinearTrueQuant

      - source_module: LiteMLMatmul
        destination_module: QuantMatmul
        params:
          quantizer_in1: # per token per head
            act_bits: 8
            dims: [ 0, -1]
          quantizer_in2: # per tensor per head
            act_bits: 8
            dims: [ 0, -1, -2 ]
