QAT:
    fully_quantized: False
    device: 'cuda'
    data_quantization:
      status: On
      bits: 8
      custom_bits: {}
      symmetric: Off
      per_channel: True
      quantization_mode: 'dynamic'
      observer: 'MinMax'
    weights_quantization:
      status: On
      bits: 8
      custom_bits: {}
      symmetric: On
      per_channel: False
      quant_and_freeze: True

    matmul_quantization:
      #  input_1     x   input_2    =   output
      # 4D case:
      # (b, h, m, n) x (b, h, n, p) = (b, h, m, p)
      # (b, h, m, 1)   (b, h, 1, 1) <- scale factors
      # 3D case (no heads):
      # (b, m, n) x (b, n, p) = (b, m, p)
      # (b, m, 1)   (b, 1, 1) <- scale factors
      input_1: # per token per head
        bits: 8
        symmetric: Off
        dims: [ 0, -1 ]
      input_2: # per tensor per head
        bits: 8
        symmetric: Off
        dims: [ 0, -1, -2 ]

#SmoothQuant:
#  target_layers: {'input_layernorm': ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj'],
#                     'post_attention_layernorm': ['mlp.up_proj', 'mlp.gate_proj']}
#  alpha: 0.85
#  scales_path: 'act_scales/llama-2-7b-hf.pt'