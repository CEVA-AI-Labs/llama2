QAT:
  device: "cuda"
  fully_quantized: False
  data_quantization:
    status: On
    bits: 8
    custom_bits: {}
    symmetric: On
    quantization_mode: static
    observer: "MovingAverage"
    per_channel: False

  weights_quantization:
    status: On
    bits: 8
    custom_bits: {}
    symmetric: On
    per_channel: True

  matmul_quantization:
    #  input_1     x   input_2    =   output
    # 4D case:
    # (b, h, m, n) x (b, h, n, p) = (b, h, m, p)
    # (b, h, m, 1)   (b, h, 1, 1) <- scale factors
    # 3D case (no heads):
    # (b, m, n) x (b, n, p) = (b, m, p)
    # (b, m, 1)   (b, 1, 1) <- scale factors
    input_1: # per token per head
      bits: 16
      symmetric: Off
      dims: [0, -1, -2] # override per channel per head if dims is available
#        per_token: True
    input_2: # per tensor per head
      bits: 16
      symmetric: Off
      dims: [0, -1, -2] # override per channel per head if dims is available
#        per_token: True
SmoothQuant:
  target_layers: {'input_layernorm': ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj'],
                     'post_attention_layernorm': ['mlp.up_proj', 'mlp.gate_proj']}
  alpha: 0.85
  scales_path: 'act_scales/llama-2-7b-hf.pt'